
## 专栏前言

本专栏的深度学习内容大部分基于 [山东大学计算机科学与技术学院](https://www.cs.sdu.edu.cn/) 崔学峰老师在寒假期间为大一新生开设的深度学习自修课程，此课程使用 Python 编程语言与其深度学习包以及老师提供的 Ubuntu 服务器，会相应补充关于服务器、环境配置等课程不会教授的内容，以及非计算机专业的同学学习此课程可能需要了解的一些计算机科学的基础知识，旨在帮助大家更好的学习。

有关计算机科学的基础知识主要根据一些专业资料与本人对相关知识的理解，若有不当之处，对文章内容有疑问，请留言或者通过博客下方的邮箱和账号联系我。

面向的对象：

- 计算机专业对深度学习感兴趣的大一学生
- 非计算机专业，出于兴趣或工作需要了解计算机基础知识或深度学习的人

本专栏的写作目的是帮助理解计算机科学与深度学习的内容，以及掌握其中的关键部分，而不是科普。所以在文章中我会给出大部分专业名词的严格定义（对于一些高中就学过的概念不会再作解释），非计算机专业的同学初次接触可能会觉得有困难，这些都是正常的，知识只有不断回顾，才能在你脑中留下深刻印象。

> 💡 **NOTE**: 下面有一些技巧可能帮助你阅读以及理解本专栏的内容：
> 
> - 请关注给出的专业名词的英文。一些名词的中文翻译有可能无法准确地描述其含义，并且多个不同名词可能存在相同的中文翻译或中文简称，然而它们是不同的东西。英文可以帮助你记住并理解它们之间的不同
> - 阅读专业名词定义时，建议首先找到宾语，即“它属于什么”，然后再看定语，即描述其特殊性的内容。比如：
>   - 形式语言：一个字母表上的某些有限长字符串的集合
>   - 形式语言是集合这一概念中的一个子集
>   - 形式语言这个子集中的元素必须是一个字母表上的某些有限长字符串
> - 如果你遇到了不理解的内容，可以暂时跳过，一些对于初学者来说较为深入的讲解并不会影响你对后续内容的理解，并且你不需要掌握所有内容。在学习计算机的过程中，你会对很多概念逐渐熟悉起来。

下面是写给对计算机不太了解的非计算机专业同学的计算机科学基础知识，有基础的同学可以选择性阅读。

## 计算与语言：尝试解决与描述问题的语法

### 计算是为了解决问题，但不能解决所有问题

什么是计算？为什么要计算？什么是计算机？什么是计算机科学？

在这里我不会给计算（Compute）下一个严格的定义，区别于通常在数学中所说的计算（Calculate），计算机科学中的 *计算（Compute）* 可以被认为是 **执行一个算法的过程**，它包含了数学中的计算概念。

这里有几个值得注意的地方：

- 这里使用的名词 *算法* 并没有给出对应的英文，在下文中你会看到两种算法的名词，而这里所说的算法同时包括它们。这里的算法，可以被理解为一些 **指令的序列**，简单而言，指令就是告诉你（或计算机）这一步应该做什么事情。一份菜谱也可以被认为是指令的序列，它告诉你完成一道菜需要先做什么，再做什么，最后做什么。但我们并不保证算法的“正确性”，也就是说一个算法可能是“错误”的。在这个意义上，使用 **“过程”** 或者 **“步骤”** 来表述这里的算法可能更为合适。
- 执行算法的可能是人，也可能是计算机。在以前只有人可以执行算法，而计算机的诞生与发展大大提高了计算的效率，计算机最擅长的就是快速处理大量繁重的任务，使以前许多不可能的任务成为可能，使人类可以从许多无聊的工作中解脱出来。~~所以 21 世纪了小学生为什么还需要做大量无用的算数题。~~

什么问题是可以解决的？什么问题是无法解决的？这个问题相当于：
什么问题是可以计算的？什么问题是不可计算的？

~~比如我们在幼儿园就学过的一个问题~~：已知三角形的底边长和高长，求三角形的面积？

这个问题使用 **数学公式** 就可以解决，让我们先来把这个问题（Problem）描述成一个标准形式：

- 输入（Input）：三角形的底边长 $l$，高长 $h$（2 个参数）
- 输出（Output）：三角形的面积 $S$

使用公式 $S=\frac{1}{2}lh$ 就可以解决这个问题，进一步地，我们可以把这个公式描述成一个函数：

$$
S=f(l, h)=\frac{1}{2}lh
$$

可以看到函数的参数即问题的输入，函数的值即问题的输出。

但是数学公式并不能解决所有的问题，比如我想知道某个路径，比如 `C:\Users\<user-name>\Roaming` 下的所有文件，这时就需要比公式更强大的算法。

在程序设计中通常将这一算法或称过程（Process）描述成类似数学中函数的形式，通常编程语言中也称其为函数（Function）

$$
output=myProcess(input_1, input_2, ...)
$$

特别地，过程可以没有输出，只是执行一些工作。

如下图所示，可以把过程理解为一个黑箱，将输入放进黑箱中得到输出。黑箱中执行的转换操作就是算法/过程

<img src="/img/beginner/function.png" width = "450" height = "400" alt="function" align=center />

计算能够解决（Solve）很多问题！但不是全部，比如“明天天气如何？”。尽管你可以方便地获取天气预报，然而我们都知道天气预报~~很多时候都~~不准——换句话说，它不保证给你正确的结果。但是所谓 *解决（Solve）* **一类** 问题，我们必须对此类问题 **所有可能的输入** 得到 **正确** 的结果（输出）！

相应地，在计算机科学中，问题被分为可计算的与不可计算的。*可计算（Computable）问题* 指的就是可以找到 **解决（Solve）** 这类问题的算法的所有问题类的集合，通常这类能够解决问题的算法被称为 **算法（Algorithm）**。另一个相对的算法概念是 **启发式算法（Heuristic）**，虽然我们经常看到有人使用算法来描述它们（都包括在本文一开始提到的算法概念中），但是 algorithm 和 heuristic 是有本质不同的！Algorithm 的其中一个基本要求是 **正确性**，而 heuristic 并不能保证得到正确的结果！这门课讲授的深度学习中的算法都属于 heuristic。

> 💡 **NOTE**: 算法（描述）乃至程序（实现）的正确性可能是很难证明的！甚至就如上文所述，有些问题本身就是不可计算的，比如深度学习，作为人工智能当下大热的一个技术，致力于（从概率意义上）得到很多 **不可** 计算的问题的结果，或者被应用于加速得到一些可计算但 **难** 计算问题结果的过程（而不一定正确）。

到这里你应该对计算是什么有了一个大概的印象，但是为什么要计算？从前面的介绍中你可以总结出其中一个原因——因为有些问题是纯粹的数学或其他自然科学、工程学无法解决的。

还有一些问题是数学和其他自然科学、工程学尚未找到解决方法的（比如微分方程的解析解），我们可以借助现代的计算机来进行大量复杂的计算得到近似结果（比如微分方程的数值解），很多时候近似结果是足够好的。

当然，只有计算仍然有更多的问题仍然无法解决，我们需要心理学、文学、人类学等更丰富的学科。而现在以人工智能和数据科学为代表的诸多计算机领域也逐渐向多学科交叉融合的方向发展，通常许多工程如机器翻译都需要计算机科学家与工程师和其他专业领域，如语言学、社会学、艺术学的专家合作，在这些特定领域的知识也称为 domain knowledge。

### 计算机科学的语言：形式语言

要认识计算机科学，首先要了解的概念就是 *语言（Language）*。计算机专业大一新生入学的第一门专业基础课就是程序设计语言，即通常说的编程语言。

日常生活中我们提到语言通常是指 *自然语言（Natural Language）*，即平时交流使用的语言，这类语言不是人为设计（虽然有人试图强加一些规则）而是自然进化的。而计算机科学中的语言通常指 *形式语言（Formal Language）*，是用 **精确** 的数学或机器可处理的公式 **定义** 的语言。编程语言就是一种形式语言，编程语言及其相关概念会在下文中详细解释。

> 💡 **NOTE**: 在计算机的某些领域，如 *自然语言处理（Natural Language Processing，NLP）* 中，会指出我们处理的对象即自然语言，而不是一些其他领域中研究的的形式语言或者编程语言

计算机的一个基础研究方向 *理论计算机科学（Theoretical Computer Science，TCS）* 中 *计算理论（Theory of Computation TOC）* 分支的 **研究对象** 就是语言，更准确地说是形式语言。

形式语言一般有两个方面：*语法（Syntax）* 和 *语义（Semantics）*。专门研究语言的语法的数学和计算机科学分支叫做形式语言理论，它只研究语言的语法而不致力于它的语义。

在形式语言理论中，*形式语言* 是一个**字母表上**的某些**有限长字符串**的**集合**，这个集合本身可以是无限的，而其中的字符串长度必须是有限的。

一个 *字符串（String）* 就一个是**零个或多个字符**组成的**有限序列**，与集合相区别，序列中的元素（这里就是字符）必须是有序的。而字符必须是在字母表中定义的字符，*字母表（Alphabet）* 是一个字符集合，此概念类似于英语中的字母表，但是这里说的字母表可以（并且必须！）包含任意你在语言中需要的字符（也可以自定义），比如空格、?、$、:、甚至😅、😘和🤡（只要你需要用到它们）。在个形式语言的意义上，自然语言中的标点符号也属于形式语言字母表中的字符。当你想要构造一个语言的时候，字母表是你首先应该考虑的东西：我的语言中使用哪些符号？

> 💡 **NOTE**: 零个字符组成的字符串通常称为 *空字符串* 或简称 *空串*，用符号 $\epsilon$ 表示

***

- 字母表的例子：
  - {0, 1, 2, 3, 4, a, b, c, &, *, o}
  - {🐮, 羊, 虎, ￥}

- 字符串的例子：
  - abc*70-p
  - I can do it!
  - 哔哩哔哩 (゜-゜)つロ 干杯~-bilibili

- 语言的例子（为方便阅读，采用使用""将字符串包围起来的约定）：
  - {"He was like a cock who thought the sun had risen to hear him crow.", "I wandered lonely as a cloud.", ...}
  - {"言必信，行必果。", "见贤思齐焉，见不贤而内自省也。", "益者三友：友直、友谅、友多闻。", ...}
  - {"adijo11,ndmf@!a", "passwd"}

***

形式语言这一看似简单的定义表达了甚至包括自然语言在内的所有语言的一个通用精确定义。比如使用中文表达所有可能的句子就组成了中文这一语言（则这一形式语言当然是一个无限的集合！）。

> 💡 **NOTE**: 可以把 TOC 的研究内容与代数做一个对比：TOC 的研究对象是 *语言*，代数的研究对象是 *数*，在数上可以进行加减乘除以及更复杂的运算，而对于语言我们仍然可以进行其他定义形式的运算（比如对集合的运算并交补等，因为一个语言就是一个集合！）。事实上无论是集合及其操作还是语言及其操作，如果你了解过抽象代数（近世代数）这门学科，就马上能明白它们都是代数的一种，我们通常所说的代数只是代数中一个很小的以数作为研究对象的子集。

在理论计算机科学中，形式语言常常被用来 **描述** 一类问题（Problem），比如：所有连通的无向图构成一个语言。

这样的概念对于不了解 TOC 的人来说可能难以理解：[图（Graph）](https://zh.wikipedia.org/wiki/%E5%9B%BE_(%E6%95%B0%E5%AD%A6)) 作为图论中的一种数学结构或者计算机科学中的一种数据结构，它和字符串有什么关系呢？图如何能构成一个语言呢？（注意：形式语言中的元素被规定为字符串）

> 💡 **NOTE**: 如果你不了解什么是图（Graph），只需要知道它是像多面体、函数这样有数学定义的研究对象

为了理解这一内容，下面要介绍计算机科学中一个重要的概念—— **编码** 

## 编码：理解语义

编码的概念在计算机的世界中随处可见，什么是编码？为什么要编码？如何进行编码？

### 信息 = 位 + 上下文

*编码（Encoding, Coding）* 在不同的领域有多种可能的解释，在本文中定义为将一系列抽象的概念分别以一些符号的不同排列来表示。*解码（Decoding）* 是将符号的序列按照制定的编码规则转换回其指代含义的一个过程，即编码的逆过程。而一般我们会用平时习惯使用的自然语言来表达这些抽象概念，但它们最终代表的都是同一个抽象概念。

好比双方进行私钥加密通信，你们只有共同认定一个加密与解密规则才能成功进行通信，这就是所谓的 **语义**。使用的编码规则不同，就无法理解对方表达的语义。

在著名的《深入理解计算机系统》（[CSAPP](http://csapp.cs.cmu.edu/3e/home.html)）这本书中的第一章开头提到 **“信息就是位 + 上下文”**，在这里我们可以把 **位** 就理解为 上面说的 **符号**，**上下文** 理解为上面所说的 **排列**，而 **信息** 就是我们要表达的 **语义**。（上下文就是语文阅读分析中经常出现的那个意思）

下面是一个使用文本编辑器（UTF-8 编码，一种字符编码系统）打开 WeChat.exe （就是你在电脑上登录微信时双击的那个可执行文件）的例子：文本编辑器使用 UTF-8 编码，即一种 **字符的编码** 去解释一个可执行文件的内容，即该系统上二进制 **指令的编码**，所以输出了乱码。有时你打开一些文件时出现乱码，也可能是文件使用的和打开方式使用的字符编码方式不同。

<table><tr>
    <td>
      <img src="/img/beginner/editWeChat.PNG" width = "390" height = "390" alt="使用文本编辑器打开 WeChat.exe" align=center />
      <p></p>
      <span>使用文本编辑器打开 WeChat.exe 的结果</span>
    </td>
    <td>
      <img src="/img/beginner/say.PNG" width = "400" height = "400" alt="你这说的什么猪话" align=center />
      <p></p>
      <span></span>
    </td>
</tr></table>

通常来说制定一个编码系统，你只需要简单的几步：

1. 确定你要编码的 **对象**，比如英文字母、标点符号、emoji 表情等。当然，只要你的编码方式选的好，在之后还可以在这个集合中加入新的对象。
2. 确定你的编码使用的 **符号集**，这个集合是有限的。
3. 为每个对象指定一个 **符号的排列**，即它的编码。

当然，具体的实现过程并不容易，你需要学习有关的编码理论（Coding theory）才能设计出实用的、无歧义的编码系统。

上面的 **符号集** 听起来很像语言定义中使用的 **字母表**，而 **符号的排列** 听上去也很像语言定义中使用的 **字符串**，实际上你可以认为它们就是相同的概念。现在我们可以来解释在上一部分留下的问题了——图（Graph）作为一个抽象的数学概念，我们可以指定一个编码规则，把每一个图（Graph）都表示为一个 **字符串**！这样，其中属于无向图（图集合的一个子集）的那部分对应的 **编码** 或者说 **字符串** 的集合当然构成一个语言！

像这样的 **YES-NO 问题（判定问题）** 都可以通过编码成字符串被表示成语言的形式，因为元素与集合的 **属于（YES）** 与 **不属于（NO）** 关系与判定问题有着天然的对应。因此语言可以用于 **描述** （判定）问题。

下面是计算机中的一些编码实例：

****

你在 Web 浏览器如 IE、Google Chorme 中访问某个网站输入的“网址”被称为 **统一资源定位符（Uniform Resource Locator，URL）**。它被用于定位（**L** ocate）网络资源（**R** esource），所谓网络资源，其实是存放在距离你很远的 **服务器** 上的 **各种文件**，它们可以通过 URL 这样统一（**U** niform）的访问格式来被访问（根据网站设计，这个 URL 路径可能与该文件实际在服务器上的文件路径有所不同，但形式是相同的）。无论是浏览网页，还是下载文件，你都是用 URL 来对这些资源进行访问的。

下面展示了 URL 的基本结构，其中 Protocol 是 *网络协议（Internet Protocol）*，它是一组 **通信规则**，规定了通信时信息必须采用的 **格式** 和这些格式的 **意义**。简单来讲，就是 **为了在网络中交换信息，需要遵守的一组规则**。在这个例子中使用的是 HTTPS 协议，也是我们网上冲浪时最常用的协议：超文本传输安全协议（Hypertext Transfer Protocol Secure）。Hostname 是 主机名，就是你要访问的资源所在服务器被分配的域名。Filename 是文件名（更普遍的情况是文件路径 Filepath，类似于你的计算机本地磁盘上的文件路径），也就是你要访问的 **资源** 在此服务器上的路径。通常，你只需要在地址栏中输入域名和资源路径，Web 浏览器会自动为你填充合适的协议。

<img src="/img/beginner/url_structure.png" width = "400" height = "400" alt="url 的结构" align=center />

实际上你也可以在 Web 浏览器中访问本机上的文件，~~尽管通常不会有人这样做~~，只需要在本地文件路径前添加协议 `file://`，比如 `file://C:/Users/Public`。或者直接输入本地文件路径，就如上文所述，现在的浏览器一般会在资源路径前自动为你添加合适的协议。

浏览器的地址栏可以显示中文，这是维基百科图（Graph）词条的 URL，在浏览器中打开它时，你会看到其中含有中文：

<img src="/img/beginner/encode_url_example.PNG" width = "400" height = "400" alt="地址栏中包括中文的 url" align=center />

下面来做一个实验：使用浏览器打开 [维基百科图词条的 URL](https://zh.wikipedia.org/wiki/图_(数学))（直接点击蓝字），打开后再复制地址栏中的内容——

1. 如果将地址栏的 URL 包括协议部分全部复制，就会发生转码，再重新粘贴到地址栏中，你会看到：
  `https://zh.wikipedia.org/wiki/%E5%9B%BE_(%E6%95%B0%E5%AD%A6)`
  看起来像是乱码，不过仔细观察，你很容易发现其中每一个汉字都相应地变成了连续 3 个 `%[0-F][0-F]` 的形式，其中`[0-F]` 是十六进制表示使用的数字 0-9 和字母 A-F（代表十进制中 10-15），简单来讲就是一个百分号 `%` 跟着两个数字的格式。其实这才是 URL 里中文的真实格式，你在浏览器地址栏中看到的中文是解码后的格式，便于人类阅读。因为 URL 设计之初只使用了英文字母、阿拉伯数字和某些标点符号。如果要显示其他符号和文字，则需要使用这些原有的符号来表示其他的符号和文字，这就是 **编码的一个例子**。
2. 如果只复制不包括协议的部分，结果就是你看到的字符串，比如：
  `图_(数学)`

> 😛 **JUST FOR FUN**: 域名也可以是中文，在浏览器地址栏中输入 `世界一流大学.com`，看看你会去到什么网站。

刚开始接触编程的人往往会遇到一个问题，分不清代码中的：

- `'2'`
- `"2"`
- `2`

这里使用了 C 语言中的表示方法，约定用单引号围起来的 `'2'` 是 **字符** 2，用双引号围起来的 `"2"` 是 **字符串** 2，而单独一个 `2` 就是 **（十进制）数字** 2。字符串由字符序列组成，由于目前我们讨论的是编码，所以这里只关心计算机中表示 **字符与数字的区别**。

在二进制计算机中，它们 **最终** 都是使用 0 和 1 来表示的，但编码方法不同，对 **数字** 而言是直接使用 0 和 1—— **二进制表示的数字**，而对于带有正负号的数字，计算机有特别的编码规则，用 0 和 1 来表示正号和负号，但不是简单的在数字前加上 0 和 1，这里就不作详细介绍了。而 **字符** 是使用 **数字的一些组合** 来表示的，比如一个简单的实现方式是把需要显示的字符排个队，按顺序为每一个字符指定一个数字。由于计算机中用 0 和 1 来编码数字，所以最终它们也是由 0 和 1 来表示的，但数字与字符用的编码符号不同。数字 2 代表了这个数字的抽象含义，而 字符 2 是数字 2 的一种具体表示形式，你也可以使用罗马数字符号 Ⅱ 来表示这个相同的抽象含义。如果使用 ASCII 编码，计算机中使用二进制的数字 50 来表示字符 2 ，而数字 2 就是二进制的数字 2。通常，计算机中将字符用于显示、输入输出、文本文件等，而数字用于执行计算（你在计算机中看到的“数字”，实际都是字符！计算机中真正的数字你是看不见的）。

计算机中用于表示字符的编码有很多种，除了被广泛使用的 ASCII，还有 **Unicode**——它编码了世界上大部分的文字系統。上面提到的 UTF-8 编码就是 Unicode 编码的一种实现（考虑了在二进制计算机中最终用 0 和 1 来表示的相关问题，比如存储空间）。下面是一些字符对应的 Unicode 编码（十六进制表示）：

- K：004B
- 文：6587
- 😍：1F60D

完整的 Unicode 编码可以在 [Unicode Table](https://unicode-table.com) 上找到。

> 💡 **NOTE**: 在语言的字母表举例中，也使用了字母、汉字与 emoji 表情作为字母表中字符的例子，把它们看作一个独立的、抽象的符号，是 **“不可分割"** 的。而在这里提到的字母、汉字与 emoji 表情，是为了在计算机中表示、显示它们而使用一些其他符号对其编码的例子，上面用数字对字母、汉字与 emoji 表情进行编码，这里数字就变成了编码的符号，而在二进制计算机中数字又是由 0 和 1 来进行编码的，如果讨论数字的编码，0 和 1 又变成了编码的符号。我们讨论的究竟是编码的 **对象**？还是用于编码的 **符号**？应该根据讨论的场景而定。编码是一种思维方式，对象是抽象的，符号是具体的，但在其他场景中，对象也可以变成具体的符号。

****

我们知道计算机使用的是二进制，计算机中所有的编码最终也都是用 0 和 1 表示的。编码本身与进位制无关，那为什么计算机使用的是二进制而不是我们熟悉的十进制？

### 二进制：硬件的选择

*进位制 或 进制（**Positional notation** or **positional numeral system**）* 是一种 **数字系统**，其中每一个 *数字* 被表示为一个 **数符的序列**，其 **数字（Number）的值** 是每一位 **数符（Digit）的值** 乘以由其所处位置（Position）确定的因子——**位权（Weight）** 得到的结果的总和，即：

$$
valueOfNumber = \sum_{i=0}^{n} (vauleOfDigit[i]*weight[i])
$$

> 💡 **NOTE**: 位置计数从 0 开始，而不是我们通常认为的 1，在计算机科学中计数普遍从 0 开始，当你开始编程的时候，就会对这一点深有体会。

而你选择的 *基（Radix）* （即某某进制）决定了数符的数量与位权，$r$ 进制数的数符为 $0, 1, ..., r-1$，其第 $i$ 位上的权重为 $r^i$。

为避免歧义，通常为数字加一个下标，括号中写上基，则进一步地上面的式子可以表示为：

$$
valueOfNumber_{(Radix)} = \sum_{i=0}^{n} (vauleOfDigit[i]*Radix^{i})
$$

这是一个以 Radix 为基的 $n+1$ 位 Radix 进制数的表示。当以 10 为基的时候，这就是我们中学学过的 *科学计数法*

> 💡 **NOTE**: 为什么要使用进位制？如果你不使用进位制，需要为所有的数指定一个符号，但数是无限多的，你不可能为所有数都指定一个符号。而进位制可以让你使用有限的符号（即数符 Digit，其数量等于你使用的进位制的基 Radix）来表示无限多的数。

****

一个 6 位二进制数的例子：

$$
101101_{(2)} = 1 * 2^5 + 0 * 2^4 + 1 * 2^3 + 1 * 2^2 + 0 * 2^1 + 1 * 2^0
$$

十进制数 $1125_{(10)}$：

<img src="/img/beginner/PosNotation.PNG" width = "300" height = "300" alt="进位制示意图" align=center />

如果把基换成 8 得到八进制数 $1125_{(8)}$：

$$
1125_{(8)} = 1 * 8^3 + 1 * 8^2 + 2 * 8^1 + 5 * 8^0
$$

实际上进位制表示直接给出了将其他进制数转换成十进制的方法，你可以以十进制的表示法计算八进制数 $1125_{(8)}$ 的值，看看它和十进制数 $1125_{(10)}$ 有什么区别？

****

使用相同的数符排列来表示不同的数字（含义）——这也是 **编码的一个例子**。这也是为什么要通过为数字下标加上进制来避免歧义，即指定了编码和解码的方式。通常不加下标表示默认使用十进制。

> 💡 **NOTE**: 你可能注意到这里为了方便不熟悉其他进制的同学理解，使用的基与数符的表述仍然是十进制的符号（比如上面的位权 $2^5$，严格来说应该表示为 $2^{101}$，因为 5 不是二进制的数符）。虽然表述不同，背后代表的意义是相同的。比如中文中的“奶牛”可以使用英文表述为 "cow"，但它们代指的是同一件事物。这对于理解各进制之间的转化是一关键。

> ❓ **QUESTION**:上面的进位制表示看起来非常清楚，但是仍然有一些细微的问题：当数字的位数超过了其进制的基时，会出现 **自身定义自身** 的情况。比如 3 位二进制数 $111_{(2)} = 1 * 2^{10} + 1 * 2^{1} + 1 * 2^{0}$，请注意最高位上的位权：$2^{10}$，这里 10 同样使用了进位制表示。请思考：是定义出错了吗？如果是定义出错，如何修改定义使其正确？如果定义正确，如何解释这一现象？对这个问题的回答涉及到理论计算机科学与程序设计中一个十分重要的概念，如果你没有计算机基础，可以尝试上网搜索我给出的关键信息。

> ⚠️️ **NOTICE**：另外请注意，上面的定义是对整数的定义，若考虑实数（既有整数部分又有小数部分），应该将定义更改成：
> $$
> valueOfNumber_{(Radix)} = \sum_{i=-m}^{n} (vauleOfDigit[i]*Radix^{i})
> $$
> 其中 $n, m$ 都是正数（也可能是 $\inf$）。可以理解为，位权的幂次取自整数轴：$..., -3, -2, -1, 0, 1, 2, 3, ...$

为什么现代计算机采用的不是通常我们运算用的十进制而是二进制？虽然计算机的理论模型与进位制并无关系，然而计算机的 **实现** 是一个 **工程** 问题，需要和真实的物理环境打交道。现代电子计算机用电路实现计算机模型，那就需要和物理电路打交道，需要考虑到 **稳定性**、实现复杂性、还有重要的 **成本**。

尤其在长距离传输的场景下，比如军方通信、还有我们每天都在接触的网络传输，信号的干扰与衰减是一个严重的问题。因为 $r$ 进制数需要用 $r$ 个状态来表示，假如使用电压来表示状态，需要用 $r$ 个区间来分别表示，而信号的干扰与衰减可能造成“误码”的情况，比如发送的是 3，而因为电压衰减收到的却是 2，因此对于传输的信息进行检查与纠正是一个重要的研究领域。当 $r$ 变大时，出现误码的可能性与检查、纠正的成本都会增加，对电子器件的精度和稳定性要求都要提高，电路设计的复杂性也会提升。

除了计算机使用的二进制，程序员也经常使用十六进制与八进制。因为 16 与 8 都是 2 的幂，在进制转换时非常方便，并且有时二进制数位数太多，而对应的十六进制数与八进制数位数少，方便浏览与计算。

> 💡 **NOTE**: 通常规定十六进制的数符为 0-9, A-F，其中 A-F 对应于十进制中的 10-15。

介绍完了一些简单的理论，现在我们从工程实践的角度来了解一下计算机的组成—— **现代电子计算机的硬件结构** 。

> 💡 **NOTE**: 请注意这里需要特别关注限定词：
> 
> - *现代*：历史上的计算机可能采用的是十进制而非二进制，虽然十进制同样可以用于编码与计算，但是现代计算机均采用 **二进制**，下面的讨论也基于二进制。并且现代计算机采用冯诺依曼架构，其核心思想是 **存储程序**，这是实现 **软件与硬件分离** 的关键（为什么？）。
> - *电子*：区别于 TCS 中研究前沿的量子计算机或生物计算机，这里讨论的是目前能够被制造出来并且广泛应用的电子计算机的硬件结构，其计算效率对应于计算复杂性理论中的确定性图灵机。简单来说，量子计算机与生物计算机的计算效率是现在的电子计算机的指数级别倍（是真正的指数级别！）

## 硬件：从理论到实际，从抽象到具体

现代电子计算机中所采用的电子元器件是“大规模和超大规模集成电路”。

*集成电路 或称 单片集成电路（**Integrated Circuit** or **Monolithic Integrated Circuit** ，also called **IC**、**Chip** or **Microchip**）* 是位于 **半导体** 材料（通常为硅）的一个小平板（“芯片”）上的 **一组** 电子电路。集成电路技术把一个电路中所需的电子元件及布线互连在一起，然后镶嵌在半导体基片上，然后再封裝（Packaging，可以理解为打包），成为一个具有电路功能的微型结构，这导致电路比由分立电子元件构成的电路更小、更快且更便宜——从而计算机可以从原来占用几间屋子的大型机演变为现在的个人计算机甚至嵌入式系统，比如手机。自从集成电路诞生以来，使用集成的晶体管数量来定义其规模，经历了集成电路——中规模集成电路——大规模集成电路——超大规模集成电路的发展过程（~~工程师的语言如此贫乏~~）。

> 💡 **NOTE**: 你可以认为集成电路既是一个“小型”电路又是一个“大型”电路，“小型”是指其所占物理空间很小，“大型”是指其中的电路（逻辑上）非常复杂

> 🔆 **HINT**：请注意这里提到的“封装”，“封装”即把一些东西打包，从而可以不关心其内部是如何实现的，仅关心它对外提供的接口——如何输入与输出什么、可以重复使用这一模块而不需要每次都从头实现。“封装”这一思想贯彻于整个计算机科学中，从硬件到软件，是计算机科学能够提高工作效率、减少重复劳动的关键之一。

下面是集成电路的实物照片，可以看到在芯片周围排列有长长的管脚，被称为 *引脚（Pin）*，它们就是封装好的集成电路的对外接口，用于输入输出。

<table><tr>
<td>
<img src="/img/beginner/Microchips.jpg" width = "200" height = "150" alt="Microchips" align=center />
</td>
<td>
<img src="/img/beginner/Chip-AD570JD.jpg" width = "200" height = "150" alt="Chip" align=center />
</td>
</tr></table>

想要了解更多历史，可以参考 [History of computing hardware](https://en.wikipedia.org/wiki/History_of_computing_hardware) 或 [计算机硬件历史](https://www.wikiwand.com/zh-sg/%E8%A8%88%E7%AE%97%E6%A9%9F%E7%A1%AC%E9%AB%94%E6%AD%B7%E5%8F%B2)

如果你是计算机与电子电气相关专业的同学，电路三件套（电路原理、模拟电子技术、数字电子技术）会带你走向~~挂科~~电子电路的神奇世界。

### 算术逻辑单元：计算的最小动作

我们首先从可以执行 **基本逻辑运算** 的 *门（Gate）电路 或称 逻辑门（Logic Gate）电路* 开始介绍。通常而言 *基本逻辑运算* 指的是三种逻辑运算：**与**（同时为真）、**或**（至少一个为真）、**非**（和你反着来）。

门电路是现代电子计算机的 building blocks ，就像建造大楼的砖块。

> 💡 **NOTE**: 逻辑运算的变量值仅为真（True）和假（False）两种真值（Truth Values），其操作数为 True 或 False 中的一种，结果也为 True 或 False 中的一种。做一个对比，（定义在自然数范围上的）算术运算的变量值为自然数，其操作数为一个自然数，结果也为一个自然数。如果你了解抽象代数，可以知道真值集合与逻辑运算同样构成一个代数系统，称为逻辑代数或布尔代数。

下面是一个 *非门（Not Gate）* （又称反相器，Inverter）的例子：

这是非门的电路符号，像这种约定俗成的符号就是工程师的“语言”，即使两个工程师说不同的（自然）语言，它们同样可以通过电路图（或工程图）来“交流”。这种约定在日常生活中也很常见，~~比如你看到 "yysy" 就知道这是“有一说一”的意思~~（在初中物理的电路中你也见过，比如用小方块代表电阻，两条等长的竖线表示电容）

<img src="/img/beginner/NOT_gate.png" width = "200" height = "150" alt="NOT gate symbol" align=center />

你同样可以把它看成一个“函数”，其输入逻辑值（又称布尔值，Boolean）假（False）或 真（True），或者是二进制的 0 或 1，其输出是输入的“反相”，一般我们使用被称为 *真值表（Truth Table）* 的方式来描述逻辑门的输入与输出：

| 输入 $A$ | 输出 $Y$ |
| -------- | -------- |
| Flase | True |
| True | False |

或者：

| 输入 $A$ | 输出 $Y$ |
| -------- | -------- |
| 0 | 1 |
| 1 | 0 |

> 💡 **NOTE**: 在二进制计算机中 0 和 1 可以被用来代表逻辑值 False 和 True，但是表示 **数值** 的 0 和 1 与表示 **逻辑值** 的 0 和 1 是完全不同的东西！而在计算机的具体电路中，又是使用高电平（低电平）和低电平（高电平）来表示 0 和 1 的。

上面的非门符号表示的是一个抽象的概念，真值表只是描述了非门的行为，定义了非门的输入输出接口规范，而其具体电路 **实现** （看看就行）可能有多种，比如最简单的开关电路（就是初中物理实验用的那种手动开关）

<img src="/img/beginner/NOT_simple.png" width = "150" height = "100" alt="开关实现的非门" align=center />

然而在计算机的电路中我们不可能手动进行输入，集成电路的微型结构也不允许。我们需要可以通过其他电路产生的输出（然后被输入到反相器）得到结果的实现方式，也即通过高低电平驱动~~而非人工驱动~~，下面有一些实际中会采用的电路，它们都应用了其他电子元器件比如 NMOS、三极管的特性。

<table><tr>
    <td>
      <img src="/img/beginner/NOT_NMOS.png" width = "150" height = "150" alt="NMOS 实现的非门" align=center />
      <p></p>
      <span>使用 NMOS 实现的非门</span>
    </td>
    <td>
      <img src="/img/beginner/NOT_Transistor.png" width = "250" height = "250" alt="三极管实现的非门" align=center />
      <p></p>
      <span>使用三极管实现的非门</span>
    </td>
</tr></table>

> 🔆 **HINT**：请注意 **接口规范** 与 **具体实现** 的区别，与封装这一思想同样被广泛应用于计算机科学的各个领域。同一规范（标准）可能有很多不同的实现，比如上面的逻辑门，其它的例子有编程语言，同一编程语言有不同实现方式的编译器（或解释器）。接口规范只告诉你“做什么”，而不会告诉你“怎么做”。

使用基本的逻辑门，我们可以构造复杂的系统实现更复杂的逻辑运算乃至 **算术运算** 与其他功能，上面提到的集成电路上就有大量的门电路。

计算机系统里速度最快的 *中央执行单元（Central Processing Unit, CPU）* 中负责执行各种逻辑运算与算术运算的部分就是 *算术逻辑单元（arithmetic logic unit，ALU）*，大量复杂的运算最终都被分解为在特定计算机系统中规定好的这些基本运算（属于计算机指令系统的一部分，有关指令的内容会在下文作详细介绍），而 ALU 就是由门电路搭建的。

> 💡 **NOTE**: ALU 是 CPU 的两大核心部件之一，另一核心部件是 *控制单元（Controller Unit，CU）*。简单来说，控制单元决定下一步应该做什么（发出命令的人），而算术逻辑单元负责执行（其中的运算部分）（执行命令的人）。

下面是一个逻辑门实现基本算术运算的例子：一位全加器，其功能是输入只有一位的两个二进制数 $A$ 与 $B$ 相加，与低位来的一位进位 $C_{in}$，输出该位的计算结果 $S$ 与应该向高位的进位 $C_{out}$。把 $n$ 个一位全加器依次相连，每一个一位全加器向高位的进位输出 $C_{out}$ 连接到下一个一位全加器的低位进位输入 $C_{in}$，就可以实现两个 $n$ 位二进制数的相加。实际上这就是小学学过的加法计算过程。而使用门电路，你还可以设计三个二进制数相加、四个二进制数相加的电路（但必须是固定个数的二进制数相加！）

<table><tr>
    <td>
        <img src="/img/beginner/1bit_FullAdder_Symbol.PNG" width = "250" height = "250" alt="1 位全加器的符号" align=center />
        <p></p>
        <span>1 位全加器的符号</span>
    </td>
        <td><img src="/img/beginner/1bit_FullAdder_Circuit.PNG" width = "420" height = "420" alt="1 位全加器的电路" align=center />
        <p></p>
        <span>逻辑门实现的 1 位全加器的电路图</span>
    </td>
</tr></table>

> 💡 **NOTE**: 为什么可以使用基本逻辑运算的门电路来实现算术运算呢？一个原因是上面提到的计算机使用 1 和 0 分别代表 True 和 False，从而同样的 1 和 0 在逻辑运算中可以被看做逻辑值，而在算术运算中可以被看做数值（不同语义！）。而另一个原因需要你了解逻辑代数与数字电路的基础知识。总之你现在只需要知道，逻辑运算与算术运算的输入输出值使用的符号形式（0 和 1）是相同的，而其中复杂的实现交给工程师就好了。

一台主频为 $2 GHz$ 的计算机，每秒最多能够进行的基本（逻辑与算术）运算次数是 20 亿次。在 Windows 操作系统上右击开始图标，选择 Windows PowerShell，在弹出的命令行界面中执行 `systeminfo` 命令，可以在处理器这一项中找到你的电脑 CPU 主频。

> 💡 **NOTE**: 通常在通信领域使用的频率单位换算以 10 为基：$1 KHz = 10^3 Hz$, $1 MHz = 10^3 KHz$, $1 GHz = 10^3 MHz$。而在计算机中数据计量使用的单位比特 $B$（Byte，1 Byte = 8 bit，这里 1 bit 就代表一位数符 digit）的单位换算以 2 为基：$1 KB = 2^{10} B$, $1 MB = 2^{10} KB$, $1 GHz = 2^{10} MB$。
> 注意有 $10^3 = 1000$ 和 $2^{10} = 1024$。

通常由各种逻辑门构建起来的电路被称为 *组合逻辑电路（Combinational Logic Circuit）*，其特点是输出 **即时响应** 输入（逻辑上，工程中会有传播延迟的情况，请参考电磁波的传播速度）与 **无记忆性**。

然而仅仅有组合逻辑电路，与理想中的图灵机相差甚远，甚至不能实现一些很简单的任务，比如：

**把 n 个数相加，这里 n 也是待输入的变量（而不是某个固定的数）！**

使用 **存储** 可以解决这个问题。

### 存储器层次结构

*时序逻辑电路（Sequential logic Circuit）*，与组合逻辑电路相对，其输出不仅取决于输入，还取决于输入的 **顺序**，这就是 **时序（Sequential）** 的含义。

比如对于上面提到的非门而言，输入 0 永远输出 1，输入 1 永远输出 0。但是对一个时序逻辑电路，依次输入 0101 和依次输入 1010 得到的对应于不同位置上的输入 1 输出可能不同，对于输入 0 同理。

当实现了可以存储信息的电路后，我们就容易解决把任意数量的数相加的问题了，只要使用两个数的全加器每次将两个数相加，将结果 **保存** 起来，接着再使用两个数的全加器把你保存的结果与下一个数相加，结果再保存，直到所有数输入完毕。实际上，这就是你平时对一组数求和的时候做的事情（比如数学的概率统计题经常出现的求平均数）。

在工程实际中综合考虑不同存储设备的 **速度** 与 **成本**，形成了存储器的层次结构。越底层的存储设备距离 CPU 越远，存取速度越低、成本越少、容量越大。

<img src="/img/beginner/memory-hierarchy.png" width = "600" height = "500" alt="Memory Hierarchy" align=center />

> 💡 **NOTE**: 存储器层次结构的理论依据是所谓的 **局部性原理**，包括 **空间局部性** 与 **时间局部性**。*空间局部性* 是指在某一时刻访问了存储器的某个位置，则 **最近一段时间** 还可能访问 **其相邻的存储位置**，*时间局部性* 是指在某一时刻访问了存储器的某个位置，则 **最近一段时间** 还可能访问 **这个位置**。用一个形象的比喻：假如你是计算机专业的一名本科生，这学期你要上五门专业课：算法设计、数值分析、概率论、操作系统和程序设计思维。今天是周一，你只有上午一节操作系统和下午一节数值分析，你当然不会每次去上课都带上六本书！你只会在操作系统课上用操作系统的书，~~除非你想在操作系统课上补你概率论没有写完的作业~~，这就是局部性原理——在这门课的一段时间内，你只会使用到与这门课的有关的内容。
> 
> <img src="/img/beginner/book_cs.jpg" width = "320" height = "220" alt="计算机专业课本" align=center />
> 
> 使用局部性原理，我们将最近一段时间 **可能** 被使用到的数据暂时存储在更快、更靠近 CPU 的存储器中（就像你去上课前把这门课用到的书放在书包里），而大量暂时用不到的数据被长期存储在更大、更慢的存储器中（就像你把所有书都放在宿舍里），即能节约成本，又能提高计算的效率。

🦅 存储器层次结构顶端的 *寄存器（Register）* **位于 CPU 中**，是 **可以直接将数据送至 ALU** 的存储资源，访问速度最快（几乎不需要时间），容量也最小，通常被分为可以存放任意数据的通用寄存器和用于特定目的的寄存器。寄存器的存储电路是由能存储一位数据的 *锁存器（Latch）* 或 *触发器（Flip-flop）* 构成的，这种结构能够存储信号的关键是 **将输出连接到输入**，这样的行为也称为 **反馈（Feedback）**。

<table><tr>
  <td>
    <img src="/img/beginner/RS_latch.png" width = "250" height = "250" alt="一个 RS 锁存器的实现电路" align=center />
    <p></p>
    <span>一个 RS 锁存器的实现电路</span>
  </td>
  <td>
    <img src="/img/beginner/RS_flipflop.png" width = "340" height = "340" alt="一个 RS 触发器的实现电路" align=center />
    <p></p>
    <span>一个 RS 触发器的实现电路</span>
  </td>
</tr></table>

> 💡 **NOTE**: 触发器与锁存器的区别是触发器“是否存储输入”由时钟信号 CLK 来决定，而触发器总是存储输入，如上图所示。

🐆 第二层的 *高速缓存（Cache）* **位于 CPU 与下一层的 *主存（Main menory）* 之间**，是主存中数据的缓存区域。Cache 的存储电路是 *静态随机存储器（Static Random-Access Memory，SRAM）*，其存储单元同样由双稳态 **触发器** 实现。

🐎 第三层的 *主存（Main Memory）* 是存储 **运行中** 程序相关数据与指令的地方，其存储电路是 *动态随机存取存储器（Dynamic Random Access Memory，DRAM）*，而存储单元使用的是栅极 **电容**，由于电容放电（即读出数据）时会破坏数据，需要时常 **重写** 或 **刷新** 其中的数据，这就是 **动态** 的含义（寄存器与 SRAM 只要不断电不写入新的数据就可以一直保持数据）。

<table><tr>
  <td>
    <img src="/img/beginner/sram-vs-dram.jpg" width = "500" height = "500" alt="SRAM 与 DRAM" align=center />
    <p></p>
    <span>上方是 Cache 使用的 SRAM，下方是主存使用的 DRAM，即我们通常说的内存条</span>
  </td>
</tr></table>

> 💡 **NOTE**: 寄存器、Cache、主存中的数据都是为 **运行中** 程序服务的！断开电源后其中的数据会立即丢失，因此也被称为 **易失性** 存储器。

🐫 第四层的 *二级存储（Secondary Memory）* 通常也称 *外部存储* 或 *外存*，包括你计算机中的磁盘、固态硬盘等，还有像 U 盘这样的移动硬盘。其原理各不相同，在这里就不作详细的讲述了。二级存储与上面三种存储器的最大区别就是它的 **非易失性**，也就是 **数据在断电情况下仍然能够长期存储**，你安装的软件、下载保存的各种文件都在二级存储上。比如在 Windows 操作系统上你使用的资源管理器就是在访问你的二级存储（当然你也可以通过更改 url 来访问网络资源，但它们都是外部存储）。

<table><tr>
  <td>
    <img src="/img/beginner/SSD-vs-HDD.jpg" width = "500" height = "500" alt="SSD 与 HDD" align=center />
    <p></p>
    <span>两种不同的二级存储，左边是固态硬盘（SSD），右边是磁盘（HDD）</span>
  </td>
</tr></table>

在计算机中主存通常也被称为 **内存**，与外存相对，因为 CPU 与主存/内存是计算机的 **核心**，除此之外的设备包括大容量存储、键盘、鼠标、移动硬盘等，都属于 **外部设备**。请注意：日常中人们常说的内存有时并不是计算机中的内存含义，比如当一个人说“我的手机内存空间不够了，这个软件安装不了”，实际上他说的内存应该是外存，因为安装的软件（程序文件）都是位于断电不丢失的外部存储上的！通常你电脑突然死机没保存的文件都是在内存中尚未写入外存，现在很多智能的软件会帮你自动保存已更改的内容，以防止~~你~~电脑出现崩溃的情况。

> 💡 **NOTE**: 在操作系统出现后，内存被划分为 **物理内存** 与 **虚拟内存**，上文中的内存均指物理内存。

🐘 除此之外，还有远程三级存储，也就是你平时在网络上浏览和下载数据时访问的存储，它们就是在编码举例中提到过的服务器存储。

事实上，现代电子计算机的计算能力接近于计算理论上可以解决所有可计算问题的图灵机，之所以说接近，是因为实际中的计算机存储总是有限的（不管多么大），而图灵机要求的存储空间是无限的。

存储器不仅可以存储数据，还能够存储已经写好的 **程序**，上面提到 **存储程序** 就是冯诺依曼计算机架构的核心，是实现 **软硬件分离** 的关键。在早期可编程计算机中，早期计算机数据存放在存储器中，程序则是控制器的一部分，要执行一个特定的程序必须直接与硬件交互，修改程序时需要更改硬件的连接等配置，如果更改了程序后要重新运行前一个程序，又需要重复一遍工作，最重要的是直接操作硬件比起现在编写软件更加困难，出现 Bug 也更难调试，费时费力。而冯诺依曼架构的 **存储程序** 思想将程序 **以数据的形式** 存放在存储器中，再加上 **控制单元**——可以根据规则从存储器指定的位置依次取出指令并向其它部件发出执行指令的信号，从而实现了 **软件与硬件的分离**，程序员不需要再直接对硬件进行编程，且程序编写好后不用再重复劳动了。

### 控制单元与指令系统：计算机的语言

*控制单元（Control Unit，CU）* 位于 CPU 中，负责控制指令流，即程序的运行。现代计算机中控制单元通常包含一个 *程序计数器（Program Counter，PC）*，属于存储器层次结构中的 **特定目的寄存器**，用于 **存放指令在内存中的地址**，冯诺依曼计算机架构就是使用它来区分存放在内存中的哪些是指令（而不是数据）的。之所以叫 **计数器（Counter）**，是因为每次取出指令后其中地址会自动 **“+1”**，从而一个程序中的指令可以自动顺序执行（一个程序的指令在内存中顺序存放），如果在程序中出现非顺序执行的情况，计算机会有特别的处理方式，但依旧是依靠 PC 来找到下一条应该执行的指令的地址。简单来说，PC 告诉控制单元“去哪找下一条指令”。而从内存中取出来的指令也会放在一个特定的寄存器中，再通过 **解码** 过程——将用 0 和 1 编码的指令变成电路中直接的控制信号，从而 **执行** 该指令。

> 💡 **NOTE**: 请注意这里粗体强调的 **“+1”**，并不是在地址数上加上 1 这个数，而是一个指令在内存中占的“宽度”，也就是 **“加上一条指令的长度”**——到下一条指令的开头。

计算机使用的 *指令系统*，也叫 **机器语言（Machine Language）**，即使用 0 和 1 编码，计算机能够直接解释与执行的基本指令集合。它在设计 CPU 的时候决定，也就是说，计算机上的 CPU 决定了它可以执行的指令有哪些。通常来说，指令系统包括上述提到的算术运算与逻辑运算指令，以及将数据与指令在一些存储位置之间移动的的指令等等。所有复杂的计算机程序最终都是由这些看起来功能非常简单的基本指令实现的。

<table><tr>
  <td>
    <img src="/img/beginner/intel_cpu.jpg" width = "400" height = "400" alt="Intel CPU" align=center />
    <p></p>
    <span>Intel 公司制造的 Core 系列处理器</span>
  </td>
  <td>
    <img src="/img/beginner/Intel-Core_example.jpg" width = "400" height = "400" alt="Intel Core 系列" align=center />
    <p></p>
    <span>实际上处理器就是一块集成电路</span>
  </td>
</tr></table>

至此，我们已经介绍了冯诺依曼计算机体系结构（现代电子计算机通常使用的架构）的 **核心部分**：中央处理器（CPU）中的 **算术逻辑单元（ALU）** 和 **控制单元（CU）**、以及存储器体系结构中的 **主存（MM）**。它们是计算机用于 **运行** 程序（或者说 **计算**）的全部组成部分。而冯诺依曼结构的输入与输出就包括上面提到的用于长期存储（数据和程序）的外存，还有键盘、鼠标、显示器、打印机等用于输入输出（数据）的其他外部设备。

<img src="/img/beginner/Von_basic_structure.png" width = "500" height = "500" alt="冯诺依曼架构的基本结构" align=center />

命令行中可以使用 gdb 调试程序查看一个程序的机器语言代码，下图中间的 01 码就是一个程序中的机器指令序列。

<img src="/img/beginner/machine_code.jfif" width = "500" height = "500" alt="机器语言代码" align=center />

机器指令看起来很令人头疼，早期的程序员直接使用机器指令进行编程，很容易出错。现在的程序员大部分使用对人类更友好的编程语言进行编程，编写出来的程序可以达到相同的功能，在计算机上运行，可是计算机只能执行机器语言，程序员们究竟是如何做到这一点的呢？

## 编程：实现算法

编程——编写 **程序（Program）** 即使用某种编程语言来实现算法。算法是指令的序列，但这里的“指令”可以用自然语言等非形式化的语言来描述，而程序必须使用形式化的语言来描述，通常是某种编程语言标准，比如上面的机器语言、还有更高级的 C 语言与 Python 语言等。

自然语言是用于人与人之间交流的语言，你也可以认为编程语言就是用于人与计算机之间交流的语言，或者说，它是人类用来 **告诉计算机怎么做，执行什么指令** 的语言。

### 程序设计的起源

公认世界上第一个程序员是著名的英国诗人拜伦与温特沃斯女爵安娜贝拉之女—奥古斯塔·埃达·金（Augusta Ada King）。

1833 年，艾达遇到了通用计算机之父，发明家和机械工程师查尔斯·巴贝奇（Charles Babbage），他提出了差分机与分析机的设计概念，虽然是机械式计算机、但其设计具有现代计算机的所有基本要素，分析机被认为是早期计算机的雏形，巴贝奇因此被视为计算机先驱。

1842 年艾达为这部机器编写了 **算法**，1843年在英国科学期刊上发表。后人视之为第一套算法、最早的计算机程序。她在论文中介绍了如何为巴贝奇分析机创建代码，用来处理字母、符号和数字。她还为这部机器创建了一种重复一系列指令的方法，这个过程就是现在程序设计中不可或缺的 **循环**。

艾达建立了 **循环（Loop）** 和 **子程序（Subprogram，Subroutine）** 概念，为计算程序拟定 **算法**，写出了人类历史上第一份“程序设计流程图”。

> 💡 **NOTE**: 这里的 **子程序**，就是在前面提到过的 **封装** 思想在 **软件**（或者说 **程序设计**） 中的体现。

然而巴贝奇与艾达的思想太过超前，在那个时代未能成功制造出分析机。

而直到 20 世纪 30 年代，计算机科学之父，数学家、密码学家和理论生物学家艾伦·图灵（Alan Mathison Turing）和他的老师阿隆佐·邱奇（Alonzo Church）才分别提出了 **等价于算法的计算模型**：**图灵机（Turing Machine）** 与 **$\lambda$ 演算**。邱奇以及数学家斯蒂芬·科尔·克莱尼（Stephen Cole Kleene）和逻辑学家 J. Barkley Rosser 还一起定义了一类函数， 这种函数的值可使用 **递归** 方法计算。这三个理论被实践“证明”是等价的，这就是著名的邱奇-图灵论题。而 **递归** 直到现在仍然是程序设计的一个重要思想与方法。

时至今日，图灵机还是计算理论研究的中心课题。图灵证明了图灵机的 **停机问题（The Halting Problem）** 无法判定，对“是否所有问题都是可计算的”这一问题做出了否定的回答，说明了现代计算机并不是无所不能的，存在 **不可计算** 的问题，这给出了计算的局限性。现代的高级编程语言，如 C 语言、Java 语言、 Python 语言等，都是图灵完备的——都能够模拟图灵机，它们的计算能力相同。

> 💡 **NOTE**: 图灵机的停机问题指是否存在一个通用方法能够判断这样一类问题：给某一台图灵机某一个输入，它是否会 **停止运行**，也就是 **对这个输入给出一个结果**。这里的通用方法，指的就是在上文中说的可以完全 **解决（Slove）** 一类问题的 **算法（Algorithm）**。可以解决一类问题的算法（Algorithm）除了上文提到的 **正确性**，另一个重要的特点就是 **有穷性**，即这里说的 **停机**——算法对任意输入能够终止。不停机，就是 **没有得到结果**，得不到结果则结果的正确性更无从谈起，所以说对停机问题不可判定的证明指明了存在 **不可计算** 的问题，指出了计算机的能力是有限的。

早期的计算机使用 **硬件编程**。目前世界公认的第一台通用电子计算机，是 1946 年美国为二战弹道计算制造的 **ENIAC**——电子数字积分计算机（Electronic Numerical Integrator And Computer）。在 ENIAC 上进行编程需要直接与硬件打交道，手动切换凌乱不堪的线缆和重置各种开关，这项工作需要分析微分方程，然后确定如何将电线连接到正确的电路。ENIAC 的六位程序员（ENIAC Girls）同样提出了子程序、**嵌套子程序** 和 **程序模块化** 的重要思想。

到了 1945 年，现代计算机之父，数学家、物理学家和经济学家约翰·冯·诺依曼（John von Neumann）作为制造 **EDVAC**——离散变量自动电子计算机（Electronic Discrete Variable Automatic Computer）的技术顾问，在他的论文：First Draft of a Report on the EDVAC 中提出了 **存储程序** 式计算机，即冯诺依曼结构。存储程序的思想实现了软件与硬件的分离，程序员开始使用 **软件编程**。直到现在，冯诺依曼结构仍然是现代计算机通用的结构，一些其他的结构如哈佛结构，也是在冯诺依曼结构的基础上改进而来的。

<table><tr>
  <td>
    <img src="/img/beginner/Ada.jfif" width = "400" height = "400" alt="Ada 的肖像" align=center />
    <p></p>
    <span>艾达的肖像</span>
  </td>
  <td>
    <img src="/img/beginner/Alan_Turing.jpg" width = "420" height = "420" alt="16 岁的 Turing" align=center />
    <p></p>
    <span>16 岁的图灵</span>
  </td>
  <td>
    <img src="/img/beginner/VonNeumann.gif" width = "440" height = "440" alt="20 世纪 40 年代的 vonNeumann" align=center />
    <p></p>
    <span>20 世纪 40 年代的冯诺依曼</span>
  </td>
</tr></table>

当然，还有更多做出重要贡献的科学家与工程师在这里没有被提及，比如雅卡尔、帕斯卡、莱布尼茨、布尔。如果你感兴趣，可以自行了解更详细的计算机发展史。

### 从低级语言到高级语言

早期程序员直接使用机器语言来编程，只由 0 和 1 组成的机器语言虽然可以在计算机上直接执行，但是对人类却极不友好，01 码也难以记忆。为了方便程序员进行编程，汇编语言（Assembly Language）出现了，可以简单地认为汇编语言就是将 01 码组成的机器语言翻译成了类似于英语的简单形式，便于程序员记忆指令。

<img src="/img/beginner/Assembly_machine.png" width = "450" height = "450" alt="汇编语言与机器语言对照表" align=center />

汇编语言与机器语言是 **一一对应** 的，因此依赖于硬件，程序的 **移植** 非常不方便，在不同的系统上要重新编写代码，并且依赖于程序员对不同硬件的指令系统的了解。但也正因如此，汇编语言代码可以容易地转换到机器语言代码。

机器语言与汇编语言都是以计算机的“思考方式”来设计的，被统称为 **低级语言**。

那能不能以更“高级”，更接近人类思考方式而不是计算机的“思考方式”来编程呢？这就是现在程序员使用的 **高级语言**，比如上面提到的 C 语言、Python 语言，还有早期的 Pascal 语言，等等，它们与汇编语言相比，和机器语言并不是一一对应的，一行高级语言代码可能对应多条机器语言代码，因此可以实现程序的 **移植**，程序员不需要了解硬件的指令系统，软件与硬件彻底分离了。并且高级语言对人类更加友好，高级语言的代码接近于高层次（High-level，不涉及底层细节）的算法描述，与人类熟悉的自然语言更加相近。

<img src="/img/beginner/programming_code.jfif" width = "450" height = "450" alt="高级语言" align=center />

高级语言有如此多的优点，可是它不能直接在计算机上运行，这个最大的问题要如何解决呢？于是 **编译器（Compiler）** 诞生了，它是一个程序，可以将特定的高级语言转换为汇编语言，再到机器语言。编译器是 **平台依赖**（包括硬件以及后来出现的系统软件——操作系统） 的，而高级语言的语法与 **平台无关**。只要你在计算机上安装了编译器，就可以将其他平台上编写的高级语言代码移植过来，使用编译器编译成机器代码后在你的计算机上运行。这不仅解决了高级语言编写的程序不能在计算机上运行的问题，并且大大减少了程序员的工作量。

> 💡 **NOTE**: 还有另一种将高级语言代码转换成机器代码的程序，叫做 *解释器（Interpreter）*。就像 compile 和 interpret 之间的区别那样，编译器会一次性将高级语言的源代码翻译成机器指令序列，即从高级语言源代码文本文件到计算机 **可执行** 的 **二进制文件**，一次编译后可以多次运行（生成的可执行文件）。在 Windows 操作系统上可执行文件的扩展名通常为 `.exe`，Mac 操作系统使用 `.dmg` 以及 `.app` 作为可执行文件的扩展名，而 Linux 操作系统没有标准的文件扩展名；而解释器不会生成可执行二进制文件，是每一次“运行源代码”时从头开始对编程语言源代码逐行进行翻译，翻译成机器指令后让计算机执行。简单来讲，使用编译方式的编程语言的编译器（程序）运行在代码 **运行前** 或者说 **编译时**，而使用解释方式的编程语言的解释器（程序）运行在代码 **运行时**。举一个形象的例子，编译器做的工作就是翻译一本书，全部翻译工作完成后才能出版，而翻译完成后其他人可以在任意时间去阅读这本书中的内容；而解释器做的工作更像口译员，双方交流时逐句翻译，~~除非有人录音或者记录下来，否则~~在翻译结束后不可能再去知道翻译的内容，即使两个人要重复一遍一模一样的对话，也必须重新找来口译员重新开始翻译。使用解释器的高级语言可以进行交互式编程——输入一行代码可以立即执行，比如 Python 语言，还有操作系统的命令行。但是通常使用编译器的高级语言执行速度较快。

世界上第一个编译器的发明者是美国海军准将、计算机科学家和程序员格蕾丝·穆雷·霍普（Grace Murray Hopper），本名格蕾丝·布鲁斯特·穆雷（Grace Brewster Murray）。

<table><tr>
  <td>
    <img src="/img/beginner/Grace_Hopper_UNIVAC.jpg" width = "570" height = "570" alt="Grace with UNIVAC" align=center />
    <p></p>
    <span>1960 年在 UNIVAC 键盘前的格蕾丝</span>
  </td>
  <td>
    <img src="/img/beginner/Commodore_Grace_Hopper.jpg" width = "400" height = "400" alt="1984 年的 Grace" align=center />
    <p></p>
    <span>1984 年的格蕾丝</span>
  </td>
</tr></table>

1949年，格蕾丝进入埃克特-莫奇莱计算机公司，担任资深数学家，开发一台商用电子计算机 UNIVAC I，她认为可以在 UNIVAC I 上面开发程序，使程序员可以用 **接近英文写作** 的方式來编写程序。她提出开发 **高级编程语言** 的想法，自行着手研究。1951 年到 1952 年间，格蕾丝在 UNIVAC I 上开发出 **第一套编译器 A-0 系統**，能将 **高级语言源代码** 编译为 **机器代码**。

> 💡 **NOTE**: 1947 年 9 月 9 日，格蕾丝在哈佛二号计算机中內发现了一只 **虫子（Bug）**。当时哈佛二号总是出錯，大家仔细检查程序仍然找不出错误，最后才发现原来是这只飞蛾意外飞入计算机內部的继电器而造成短路。他们把虫子移除后计算机终于正常运行，并在日记本中记录下了这一事件，她在日记中使用了 "Debug" 这个单词。从此，**Debug**（“除虫”） 正式成为排除程序错误的专业术语，而 **Bug** 被用于指代程序中的错误（Error）。
> 
> <img src="/img/beginner/1st_Bug.jpg" width = "300" height = "300" alt="第一个“Bug”日记" align=center />

特定编程语言对应的编译器只是一个分析其编程语言语法的 **程序**，译器报告错误（**Error**）当且仅当源代码文件不符合编程语言的语法，这样的程序无法理解语言的语义，所以有时你会发现编译通过而调试结果与预期不符或者出现 **异常（Exception）**，因为你可能打错、漏了什么字符，常见的一种问题是把判断等于号 `==` 打成了赋值号 `=`，而现代编译器对于一些可能出现异常。相对地，在自然语言中你可能使用一些不符合语言语法的表述（~~比如倒装句~~），然而对方仍然能够理解你的意思。

所以当你发现编译器报错，或者程序调试出现 Bug 的时候，一定是你错了，或者你使用的由第三方开发者开发的软件包（Software Package）本身就有 Bug，而不是编译器或计算机（除非你是一个~~可以反向警告编译器~~的编程高手，编译器也是程序员写出来的程序，可能存在问题，但这些工具都是由众多编程高手编写与改进，经过时间检验的），请老老实实查阅相关资料，找到问题所在吧！

实践是学习计算机的最佳途径，如果你想对计算机科学有更深入的了解，动手开始学习编程是最好的方法。在学习编程的过程中，请始终记住一句话——**熟能生巧**。尽管也需要思考，但是 **大量的练习** 绝对是成为一个编程高手的 **必要条件**！

**语言** 可以被用来描述问题，也可以被用来（尝试）解决问题。描述问题与（通过 **计算**）解决问题，这就是计算机科学关注的内容。
